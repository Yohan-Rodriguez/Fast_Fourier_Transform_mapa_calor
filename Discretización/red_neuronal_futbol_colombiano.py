# -*- coding: utf-8 -*-
"""red_neuronal_futbol_colombiano

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1rih_TVwbrPNzaeln3JNkoeo6avadtwmw

# 1) **Librerías & funciones extras**
"""

import os
from math import ceil
from google.colab import drive

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import joblib

# Habilitar mostrar todas las columnas
pd.set_option('display.max_columns', None)

# Montar Google Drive
drive.mount('/content/drive')

# Ruta de la caprte principal del proyecto
path = '/content/drive/My Drive/Colab Notebooks/Algoritmos de ML/Red Neuronal Fútbol Colombiano'

'''
CARGAR MODELO Y REALIZAR PREDICCIONES
'''
def load_model_and_predict(new_data, model_filename):
    '''
    Description:
    This function loads a pretrained model from a file and makes a
    prediction on new data.

    Parameters:
    new_data: Diccionario con los datos de entrada
    model_filename: Nombre del archivo del modelo a cargar

    Returns:
    predicted_new_data_tx: Predicción del modelo
    '''
    # Cargar el modelo desde la ubicación del archivo
    # e imprimir información básica sovbre él
    model_loaded = joblib.load(model_filename)

    # Preparar data para el modelo
    inputs = [list(new_data)]

    # Usar modelo para realizar predicciones
    predicted_new_data_tx = model_loaded.predict(inputs)[0]

    return predicted_new_data_tx


'''
ESTANDARIZACIÓN O NORMALIZACIÓN DE LAS CARACTERÍSTICAS
'''
def stand_norm_data(X, y, norm=True, test_size=0.2):
  if norm:
    # Crear el objeto scaler de NORMALIZACIÓN
    scaler = MinMaxScaler(feature_range=(0, 1)).fit(X)
    # Normalizar los datos
    X_norm = scaler.transform(X)
    return train_test_split(X_norm, y, test_size=test_size, random_state=0), scaler

  else:
    # Crear el objeto scaler de ESTANDARIZACIÓN
    scaler = StandardScaler()
    # Estandarizar los datos
    X_std = scaler.fit_transform(X)
    return train_test_split(X_std, y, test_size=test_size, random_state=0), scaler

"""# 2) **DF Estadísticas del campeonato**

## 2.1 **Dataset original**
"""

'''
CONCATENAR LOS ARCHIVOS ".csv" DE CADA UNA DE LAS JORNADAS Y SUS ESTADÍSTICAS
'''
def concat_df_jornadas():
  # Listar archivos en una carpeta específica
  path_jor = '{}/Jornadas'.format(path)

  # DataFrame base
  df = pd.DataFrame()

  # Iterar sobre los archivos en el directorio
  for archivo in os.listdir(path_jor):
    # Ruta de un archivo ".csv" en particular
    ruta_archivo = os.path.join(path_jor, archivo)
    # Verificar si "ruta_archivo" es un archivo
    if os.path.isfile(ruta_archivo):
      # Verificar si el archivo es un archivo CSV
      if ruta_archivo[-3:] == 'csv':
        # Concatenar la información de los archivos ".csv"
        df = pd.concat([df, pd.read_csv(ruta_archivo, encoding='latin1', sep=',')], ignore_index=True)

  # Organizar por jornada
  df.sort_values(by='JOR', inplace=True)

  # Setear los indices de manera continua de 0 a 189
  df.reset_index(drop=True, inplace=True)

  # Guardar el DataFrame
  df.to_csv('{}/Modelos/df_base_stats.csv'.format(path), index=False)

  return df

'''
"DF" CON LAS ESTADÍSTICAS DE LOS PARTIDOS EN CADA JORNADA
'''
update_stats = False
# True = Actualizar las estadísiticas de los partidos
# False = Leer el archivo con las estad´siticas ya extraidas

if update_stats:
  df_stats = concat_df_jornadas()
else:
  df_stats = pd.read_csv('{}/Modelos/df_base_stats.csv'.format(path))

df_stats

"""## 2.2 **DF por Jornadas** [190 partidos: 10 partidos * 19 jornadas]"""

# Extraer estadísticas del equipo local
stats_h = [ i_stat.replace('[', '').replace(']', '').replace("'", '').replace("'", '').split(', ') for i_stat in df_stats['stats_match_H'] ]
# Extraer estadísticas del equipo visitante
stats_a = [ i_stat.replace('[', '').replace(']', '').replace("'", '').replace("'", '').split(', ') for i_stat in df_stats['stats_match_A'] ]

# Nombres de las nuevas columnas para el "df_stats"
names_stats = ['%_POS_BAL', 'DIS_REC', 'TAR_AMA', 'TAR_ROJ', 'FAL_REC', 'FAL_COM', 'PER_D_POS', 'REC_D_POS', 'FUE_DE_JUE', 'DIS_REC_BLO']

# Crear nuevas columnas de estadisticas por equipo en el "DataFarme"
for enu, i_name_col in enumerate(names_stats):
  # 'DIS_REC', 'TAR_AMA', 'TAR_ROJ', 'FAL_REC', 'FAL_COM', 'PER_D_POS', 'REC_D_POS', 'FUE_DE_JUE'
  if (i_name_col != 'DIS_REC_BLO') and (i_name_col != '%_POS_BAL'):
    df_stats['{}_H'.format(i_name_col)] = [ int(i_stats_h[enu]) for i_stats_h in stats_h ]
    df_stats['{}_A'.format(i_name_col)] = [ int(i_stats_a[enu]) for i_stats_a in stats_a ]
  # '%_POS_BAL'
  elif i_name_col == '%_POS_BAL':
    df_stats['{}_H'.format(i_name_col)] = [ float(i_stats_h[enu][:-1]) for i_stats_h in stats_h ]
    df_stats['{}_A'.format(i_name_col)] = [ float(i_stats_a[enu][:-1]) for i_stats_a in stats_a ]
  # 'DIS_REC_BLO'
  else:
    df_stats['{}_H'.format(i_name_col)] = [ int(i_stats_h[enu+2]) for i_stats_h in stats_h ]
    df_stats['{}_A'.format(i_name_col)] = [ int(i_stats_a[enu+2]) for i_stats_a in stats_a ]

# Agreagar estadísticas de tiros al arco (SHO_TAR_x) y tiros afuera (SHO_x)
df_stats['SHO_TAR_H'] = [int(i_sth[-3]) for i_sth in stats_a]
df_stats['SHO_TAR_A'] = [int(i_sta[-2]) for i_sta in stats_h]
df_stats['SHO_H'] = [int(i_sh[-3]) for i_sh in stats_h]
df_stats['SHO_A'] = [int(i_sa[-2]) for i_sa in stats_a]

# Crear columnas de promedio de goles para locales y visitantes con valores NaN
df_stats['AVG_GOAL_H'] = np.nan
df_stats['AVG_GOAL_A'] = np.nan
df_stats['AVG_%_POS_BAL_H'] = np.nan
df_stats['AVG_%_POS_BAL_A'] = np.nan
df_stats['AVG_SHO_TAR_H'] = np.nan
df_stats['AVG_SHO_TAR_A'] = np.nan

# Crear las nuevas columnas de forma vectorizada
df_stats['H_WIN'] = np.where(df_stats['G_H'] > df_stats['G_A'], 1, 0)
df_stats['A_WIN'] = np.where(df_stats['G_H'] < df_stats['G_A'], 1, 0)
df_stats['DRAW'] = np.where(df_stats['G_H'] == df_stats['G_A'], 1, 0)

# Etiqueta del modelo
# df_stats['RESULT'] = np.where(df_stats['G_H'] > df_stats['G_A'], 0, np.where(df_stats['G_H'] == df_stats['G_A'], 1, 2))
df_stats['RESULT'] = np.where(df_stats['G_H'] > df_stats['G_A'], 1, 0)
# 1 = WIN HOME
# 0 = DRAW o WIN AWAY

# Eliminar las columnas originales innecesarias
df_stats.drop(columns=['stats_names', 'stats_match_H', 'stats_match_A'], inplace=True)

# Organizar por jornada y alfabéticamente por nombre del equipo local
df_stats.sort_values(by=['JOR', 'HOME'], inplace=True)

# Re-setear index
df_stats.reset_index(drop=True, inplace=True)

'''
CALCULAR PROMEDIOS POR JORNADA (DE LOCAL Y VISITANTE)
'''
# 'AVG_GOAL_H', 'AVG_GOAL_A', 'AVG_%_POS_BAL_H', 'AVG_%_POS_BAL_A', 'AVG_SHO_TAR_H', 'AVG_SHO_TAR_A'
for i_status in ['HOME', 'AWAY']:
  # Obtener los nombres únicos de los equipos
  for i_team in df_stats[i_status].unique():
    # Obtener los goles en cada jornada de cada equipo cuando el equipo fue local o visitante
    list_goals = df_stats[ df_stats[i_status] == i_team ].groupby(['JOR', 'HOME'])[ str(np.where(i_status=='HOME', 'G_H', 'G_A')) ].sum().to_list()

    # Obtener la posición en cada jornada de cada equipo cuando el equipo fue local o visitante
    list_pos = df_stats[ df_stats[i_status] == i_team ].groupby(['JOR', 'HOME'])[ str(np.where(i_status=='HOME', '%_POS_BAL_H', '%_POS_BAL_A')) ].sum().to_list()

    # Obtener los tiros a puerta en cada jornada de cada equipo cuando el equipo fue local o visitante
    list_shots_tg = df_stats[ df_stats[i_status] == i_team ].groupby(['JOR', 'HOME'])[ str(np.where(i_status=='HOME', 'SHO_TAR_H', 'SHO_TAR_A')) ].sum().to_list()

    if i_status == 'HOME':
      df_stats['AVG_GOAL_H'][df_stats[i_status] == i_team] = [ np.mean(list_goals[ :enu+1 ]).round(1) for enu, i_mean in enumerate(list_goals) ]
      df_stats['AVG_%_POS_BAL_H'][df_stats[i_status] == i_team] = [ np.mean(list_pos[ :enu+1 ]).round(1) for enu, i_mean in enumerate(list_pos) ]
      df_stats['AVG_SHO_TAR_H'][df_stats[i_status] == i_team] = [ np.mean(list_shots_tg[ :enu+1 ]).round(1) for enu, i_mean in enumerate(list_shots_tg) ]
    else:
      df_stats['AVG_GOAL_A'][df_stats[i_status] == i_team] = [ np.mean(list_goals[ :enu+1 ]).round(1) for enu, i_mean in enumerate(list_goals) ];
      df_stats['AVG_%_POS_BAL_A'][df_stats[i_status] == i_team] = [ np.mean(list_pos[ :enu+1 ]).round(1) for enu, i_mean in enumerate(list_pos) ]
      df_stats['AVG_SHO_TAR_A'][df_stats[i_status] == i_team] = [ np.mean(list_shots_tg[ :enu+1 ]).round(1) for enu, i_mean in enumerate(list_shots_tg) ]

df_stats

"""## 2.2 **DF Segmentando data por fechas**"""

# Se realizará el análisis a partir de la jornada "x" a la 19
num_jornadas = 15

dict_stats = {
    'NAME_TEAM': df_stats['HOME'].sort_values().unique(),

    # 'PJ_H',	'PG_H',	'PE_H',	'PP_H'
    'PJ_H': df_stats[df_stats['JOR'] >= num_jornadas].groupby('HOME')['HOME'].count().values,
    'PG_H': df_stats[df_stats['JOR'] >= num_jornadas].groupby('HOME')['H_WIN'].sum().values,
    'PE_H': df_stats[df_stats['JOR'] >= num_jornadas].groupby('HOME')['DRAW'].sum().values,
    'PP_H': df_stats[df_stats['JOR'] >= num_jornadas].groupby('HOME')['A_WIN'].sum().values,

    'PJ_A': df_stats[df_stats['JOR'] >= num_jornadas].groupby('AWAY')['AWAY'].count().values,
    'PG_A': df_stats[df_stats['JOR'] >= num_jornadas].groupby('AWAY')['A_WIN'].sum().values,
    'PE_A': df_stats[df_stats['JOR'] >= num_jornadas].groupby('AWAY')['DRAW'].sum().values,
    'PP_A': df_stats[df_stats['JOR'] >= num_jornadas].groupby('AWAY')['H_WIN'].sum().values,

    # GF_H	GC_H
    'GF_H': df_stats[df_stats['JOR'] >= num_jornadas].groupby('HOME')['G_H'].sum().values,
    'GC_H': df_stats[df_stats['JOR'] >= num_jornadas].groupby('HOME')['G_A'].sum().values,
    'GF_A': df_stats[df_stats['JOR'] >= num_jornadas].groupby('AWAY')['G_A'].sum().values,
    'GC_A': df_stats[df_stats['JOR'] >= num_jornadas].groupby('AWAY')['G_H'].sum().values
 }

'''
Crear DataFrame temporal
'''
# for i, j in dict_stats.items():
#   print(i, len(j))

df_temp_stats = pd.DataFrame(dict_stats)

# Calcular los puntos de local y visitante
df_temp_stats['PTS_H'] = df_temp_stats.apply(lambda x: x['PG_H']*3 + x['PE_H']*1, axis=1)
df_temp_stats['PTS_A'] = df_temp_stats.apply(lambda x: x['PG_A']*3 + x['PE_A']*1, axis=1)
df_temp_stats['PTS_T'] = df_temp_stats['PTS_H'] + df_temp_stats['PTS_A']

df_temp_stats['PJ_T'] = df_temp_stats['PJ_H'] + df_temp_stats['PJ_A']



def cal_porcentajes(df):
  # Porcentajes de los equipos en condición de local
  df_porcentajes = df[['NAME_TEAM', 'PJ_H',	'PG_H',	'PE_H', 'PP_H']].copy()
  df_porcentajes['%_PG_H'] = df_porcentajes.apply(lambda x: round(x['PG_H'] * 100 / x['PJ_H'], 2), axis=1).copy()
  df_porcentajes['%_PE_H'] = df_porcentajes.apply(lambda x: round(x['PE_H'] * 100 / x['PJ_H'], 2), axis=1).copy()
  df_porcentajes['%_PP_H'] = df_porcentajes.apply(lambda x: round(x['PP_H'] * 100 / x['PJ_H'], 2), axis=1).copy()

  # Porcentajes de los equipos en condición de visitante
  df_porcentajes[['PJ_A',	'PG_A',	'PE_A',	'PP_A']] = df[['PJ_A', 'PG_A', 'PE_A', 'PP_A']].copy()
  df_porcentajes['%_PG_A'] = df_porcentajes.apply(lambda x: round(x['PG_A'] * 100 / x['PJ_A'], 2), axis=1).copy()
  df_porcentajes['%_PE_A'] = df_porcentajes.apply(lambda x: round(x['PE_A'] * 100 / x['PJ_A'], 2), axis=1).copy()
  df_porcentajes['%_PP_A'] = df_porcentajes.apply(lambda x: round(x['PP_A'] * 100 / x['PJ_A'], 2), axis=1).copy()

  return df_porcentajes


# Calcular porcentajes de los partidos de local y visitante
df_porcentajes = cal_porcentajes(df_temp_stats)
df_temp_stats[df_porcentajes.columns.to_list()] = df_porcentajes

# Re-úbicar columnas en posiciones nuevas
columns = df_temp_stats.columns.to_list()
columns_copy = columns.copy()

columns_copy.pop(-14)  # GF_H
columns_copy.pop(-13)  # GF_A
columns_copy.pop(-10)  # PTS_H
columns_copy.pop(-9)  # PTS_A
columns_copy.pop(-8)  # PTS_T
columns_copy.pop(-7)  # PJ_T
columns_copy.pop(-6)  # %_PG_H
columns_copy.pop(-5)  # %_PE_H
columns_copy.pop(-4)  # %_PP_H

columns_copy.insert(1, columns[-8]) # PTS_T
columns_copy.insert(2, columns[-7]) # PJ_T
columns_copy.insert(3, columns[-10])  # PTS_H
columns_copy.insert(8, columns[-14])  # GF_H
columns_copy.insert(9, columns[-13])  # GF_A
columns_copy.insert(10, columns[-6])  # %_PG_H
columns_copy.insert(11, columns[-5])  # %_PE_H
columns_copy.insert(12, columns[-4])  # %_PP_H
columns_copy.insert(13, columns[-9])  # PTS_A

# DataFarme re-organizado
df_temp_stats = df_temp_stats.reindex(columns=columns_copy)

# Ordenar el dataframe por la columna "PTS_T" de mayor a menor
df_temp_stats.sort_values(by='PTS_T', ascending=False, inplace=True)

# Re-setear índices
df_temp_stats.reset_index(drop=True, inplace=True)

df_temp_stats

"""# 3) **DF Heat Maps**

##  3.1 **Gráfica discretizada del mapa de calor**
"""

# Ancho de de la imagen
num_px_col = 800

# Función para crear el dataframe y crear la imagen de una fecha en específico
def graph_img_bin(df, num_div, name_csv):

  # Número de divisiones (# de filas) a tener en cuenta para gráficar cada punto
  num_div = num_div

  # Almacenar la suma de pixeles tomados como blancos en cada división
  list_sum_divisiones = []

  # Iterar sobre el rango de divisiones establecido de columas a sumar
  for i, j in zip(list(range(0, num_px_col, num_div)), list(range(num_div, num_px_col, num_div))):
    # Almacenar las suma de cada división y almacenarla en la lista "list_sum_divisiones"
    list_sum_divisiones.append(df[[i_col for i_col in df.columns[i:j]]].sum().sum())

  # Gráficar
  plt.figure(figsize=(12, 6))
  sns.lineplot(list_sum_divisiones, color='orange')
  plt.gca().set_facecolor('g')
  plt.title('Intensidad de juego de: {}'.format(name_csv[101:-4]))
  plt.xlabel('Cancha')
  plt.ylabel('Intensidad de juego')

  # Agregar asintotas verticales
  plt.axvline(x=400/num_div, color='red', linestyle='--')
  plt.axvline(x=100/num_div, color='blue', linestyle='solid', alpha=0.5)
  plt.axvline(x=700/num_div, color='blue', linestyle='solid', alpha=0.5)

  plt.grid()
  plt.show()

  # Guardar la imagen
  # plt.savefig('/content/{}.png'.format(df))

  return df

"""## 3.2 **Suma de actividad por regiones en** "archivo_bin.csv"
"""

def sum_por_regiones(df):
  # Ruta del directorio que contiene los archivos discretizados de los mapas e calor
  directorio = '/content/drive/My Drive/Colab Notebooks/Algoritmos de ML/Red Neuronal Fútbol Colombiano/Archivos bin/'

  # Obtener la lista de los nombres de los archivos ".csv" en el directorio
  archivos_bin = os.listdir(directorio)

  # Función personalizada para dividir la cadena y obtener el número
  def obtener_numero(cadena):
      return int(cadena.split('_')[0])

  # Ordena la lista utilizando la función "obtener_numero"
  archivos_bin = sorted(archivos_bin)

  # Ahora ordenar en orden alfabético
  archivos_bin = sorted(archivos_bin, key=obtener_numero)

  dict_model = {'JOR': [], 'HOME': [], 'AWAY': [], 'UMBRAL': [], 'IMG_OF':[],}
                # 'CAMPO_PROPIO [%]': [], 'CAMPO_RIVAL [%]': []}

  '''
  CREAR NUEVAS LLAVES DEL DICCIONARIO "dict_model" DE LAS UMAS DE REGIONES
  '''
  # Divisiones por filas y columnas
  div_row = 4
  div_col = 4

  for i_new_key in range(div_row*div_col):
    dict_model[i_new_key] = []

  # Almacenar los DataFrames de los archivos bin "archivo_bin.csv" (760)
  dict_dfs = {}

  '''
  Iterar sobre cada archivo .csv de loas binarios de
  la transformada inversa obtenidos de cada imagen
  '''
  for enu, i_bin in enumerate(archivos_bin):

    # Ruta al archivo ".csv" específico
    path_csv = directorio + i_bin

    # Crear DataFrame del "archivo_bin.csv"
    df_bin = pd.read_csv(path_csv, header=None)

    # Gaurdar df dentro del diccionario de df's
    dict_dfs[enu] = df_bin

    '''
    GRÁFICOS DE LA DISCRETIZACIÓN DE LOS MAPAS DE CALOR
    '''
    # graph_img_bin(df=df_bin, num_div=1, name_csv=path_csv)

    '''
    EXTRAER DATA DEL "str" CON EL NOMBRE DEL ARCHIVO "archivo_bin.csv"
    '''
    # String con únicamente la información útil del nombre del archivo
    str_name_file = i_bin[:-4]
    # Ej: str_name_file = '1_Deportivo Pereira_Deportivo Cali_A_bin_065'

    # El nombre del archivo ".csv" solo tiene una coincidencia de cero "0"
    index_cero = np.where(str_name_file[-3:].index('0')==0, -3, -2)

    # Sub-string con únicamente los nombre de los equipos
    names_teams = str_name_file[np.where(str_name_file[1]=='_', 2, 3):index_cero-7].split('_')
    # Ej: names_teams = 'Deportivo Pereira_Deportivo Cali' == ['Deportivo Pereira', 'Deportivo Cali']

    # Equipo al que le pertence el actual "archivo_bin.csv"
    home_or_away = np.where(str_name_file.rfind('_H_') == -1, 0, 1)
    # HOME = 1 -&- AWAY = 0

    '''
    DATA PORCENTUAL
    '''
    # Calcular la posesión del equipo, al que le corresponde el "archivo.csv", en campo propio
    # (Suma de 400 columnas)
    list_ones_own_bin = np.where(home_or_away==1, [i for i in df_bin.columns if i < 400],
                                                [i for i in df_bin.columns if i >= 400])
    sum_ones_own = df_bin[list_ones_own_bin].sum().sum()

    # Calcular la posesión del equipo, al que le corresponde el "archivo.csv", en campo rival
    # (Suma de 400 columnas)
    list_ones_vs_bin = np.where(home_or_away==1, [i for i in df_bin.columns if i >= 400],
                                              [i for i in df_bin.columns if i < 400])
    sum_ones_vs = df_bin[list_ones_vs_bin].sum().sum()

    # Posesión total del equipo cancha completa
    sum_ones_total_bin = sum_ones_own + sum_ones_vs

    # Porcentaje posesión del equipo en campo propio
    por_sum_ones_total_bin = sum_ones_own * 100 / sum_ones_total_bin

    '''
    OBTENER SUMA POR REGIONES DEL DF BIN
    '''
    # Dimensiones de la imagen/df_bin
    width_pixels = 800
    height_pixels = 524

    # Valores inciales para los parámetros de fila para ".iloc"
    delta_pix_row = height_pixels // div_row
    down_row = 0
    up_row = delta_pix_row

    # Lista para almacenar la 32 sumas por cada una de las regiones delimitadas por "row" y "col"
    list_df = []

    # Iterar sobre el rango de filas a obtener ("div_row" filas)
    for i in range(div_row):

      # Valores inciales para los parámetros de columna para ".iloc"
      delta_pix_col = width_pixels // div_col
      down_col = 0
      up_col = delta_pix_col

      # Iterar sobre el rango de columnas a obtener (8 columnas)
      for j in range(0, (div_row*div_col), div_row):
        # Almacenar la suma de la región delimitada por "row" y "col"
        list_df.append(df_bin.iloc[down_row:up_row, down_col:up_col].sum().sum())

        # Pasar a la siguiente región delimitada por las columnas "col"
        up_col, down_col = up_col + delta_pix_col, down_col + delta_pix_col

      # Pasar a la siguiente región delimitada por las filas "row"
      up_row, down_row = up_row + delta_pix_row, down_row + delta_pix_row

    '''
    CREAR ARRAY DIMENSIONAL CON LA SUMA DE UNOS (1's) POR REGIONES
    '''
    # Crear array dimensional numpy a parti de la lista de suma de regiones "list_df"
    arr_df = np.array(list_df)
    # "(div_row * div_col)" filas y 1 columna

    # Re-dimensionar el array uni-dimensional para obtener un arrreglo multidimensional
    arr_df = arr_df.reshape(div_row, div_col)
    # "div_row" filas y "div_col" columnas

    ''' Ejmeplo de: "list_df" y  "arr_df"
    list_df = [962, 1055, 2446, 2261, 2299, 1930, 3148, 753, 1392, 1364, 2120, 1909, 1915, 3004, 1515, 3221, 2530, 2225, 529, 130,
              776, 665, 698, 2176, 682, 2459, 3707, 212, 1329, 388, 348, 1259, 502, 3105, 1470, 3227, 2638, 1559, 1634, 981]

    arr_df = array([[ 962, 1055, 2446, 2261, 2299, 1930, 3148,  753, 1392, 1364],
          [2120, 1909, 1915, 3004, 1515, 3221, 2530, 2225,  529,  130],
          [ 776,  665,  698, 2176,  682, 2459, 3707,  212, 1329,  388],
          [ 348, 1259,  502, 3105, 1470, 3227, 2638, 1559, 1634,  981]])
    '''

    '''
    ALIMENTAR EL DICCIONARIO PARA EL DF "df_model"
    '''
    dict_model['JOR'].append(int(str_name_file[:3].split('_')[0]))
    dict_model['HOME'].append(names_teams[0])
    dict_model['AWAY'].append(names_teams[-1])
    dict_model['UMBRAL'].append(float(str_name_file[index_cero:].replace('0', '0.')))
    dict_model['IMG_OF'].append(home_or_away)
    # dict_model['CAMPO_PROPIO [%]'].append(por_sum_ones_total_bin)
    # dict_model['CAMPO_RIVAL [%]'].append(100 - por_sum_ones_total_bin)

    for enu, i_new_key in enumerate(range(0, div_row*div_col, 4)):
      dict_model[i_new_key].append(list_df[enu])
      dict_model[i_new_key+1].append(list_df[enu + div_col])
      dict_model[i_new_key+2].append(list_df[enu + div_col])
      dict_model[i_new_key+3].append(list_df[enu + div_col])

    '''
    GRAFICAR MAPA DE CALOR DISCRETIZADO
    '''
    # # Crear un gráfico de matriz de colores
    # plt.imshow(arr_df, cmap='Reds', interpolation='nearest')

    # # Añadir una barra de colores
    # plt.colorbar()

    # # Mostrar el gráfico
    # plt.show()

  df_model = pd.DataFrame(data=dict_model)
  df_model = df_model.round(2)

  # Guardar el archivo ".csv" que se usará como base del modelo
  df_model.to_csv('{}/Modelos/df_model.csv'.format(path), index=False)

  return df_model

'''
"DF" CON LA INFORMACIÓN DEL MAPA DE CALOR OBTENDIA EN MATLAB
'''
new_read = False
# True = Volver a generar el el archivo ".csv"
# False = Leer el archivo ".csv" guardado en My Drive

if new_read:
  df_model = sum_por_regiones(df)
else:
  df_model = pd.read_csv('{}/Modelos/df_model.csv'.format(path))

df_model

"""## 3.3 **Unir información de mapa de calor** (umbral del 0.65) **con las estadísticas del partido**"""

umbral_model = 0.65
df_model[df_model['UMBRAL'] == umbral_model]

# DataFrame con solo los datos del mapa de calor del equipo local con un umbral de "umbral_model"
df_model_copy = df_model[ (df_model['UMBRAL'] == umbral_model) & (df_model['IMG_OF'] == 1) ].copy()
df_model_copy.reset_index(drop=True, inplace=True)

# Renombrar las columnas
df_model_copy.rename(columns={'0': '0_H',	'1': '1_H',	'2': '2_H',	'3': '3_H',	'4': '4_H',	'5': '5_H',	'6': '6_H',	'7': '7_H',	'8': '8_H',
                              '9': '9_H',	'10': '10_H',	'11': '11_H',	'12': '12_H',	'13': '13_H',	'14': '14_H',	'15': '15_H'}, inplace=True)

# Agregar las columnas del mapa de calor del equipo visitante
for i_new_col in range(16):
  df_model_copy['{}_A'.format(i_new_col)] = df_model[str(i_new_col)][ (df_model['UMBRAL'] == umbral_model) & (df_model['IMG_OF'] == 0) ].values

# Agregar las columnas de las estadísticas de cada partido. EJ:
# df.columns[3:] = ['G_H', 'G_A', '%_POS_BAL_H', '%_POS_BAL_A', 'DIS_REC_H', 'DIS_REC_A', 'TAR_AMA_H', 'TAR_AMA_A', 'TAR_ROJ_H',
#                   'TAR_ROJ_A', 'FAL_REC_H', 'FAL_REC_A', 'FAL_COM_H', 'FAL_COM_A', 'PER_D_POS_H', 'PER_D_POS_A', 'REC_D_POS_H',
#                   'REC_D_POS_A', 'FUE_DE_JUE_H', 'FUE_DE_JUE_A', 'DIS_REC_BLO_H', 'DIS_REC_BLO_A', 'SHO_TAR_H', 'SHO_TAR_A',
#                   'SHO_H', 'SHO_A', 'AVG_GOAL_H', 'AVG_GOAL_A', 'H_WIN', 'A_WIN', 'DRAW', 'RESULT']
for i_col_df in df_stats.columns[3:]:
  df_model_copy[i_col_df] = df_stats[i_col_df].values

df_model_copy.drop(columns=['IMG_OF', 'UMBRAL'], inplace=True)

df_model_copy

"""# 4) **Red Neuronal FeedFoward (FNN)**"""

# Recordar la estructura del "DF" base para el model
df_model_copy

"""## 4.1 **Librerías**"""

# !pip install tensorflow

from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler, MinMaxScaler

from tensorflow import keras
from keras.models import Sequential
from keras.layers import Dense
from keras.models import load_model

"""## 4.2 **Modelo**"""

'''
Codificar las columnas de los equipos "HOME" y "HOME"
'''
# Obtener una lista de equipos únicos
all_teams = df_model_copy['HOME'].unique().tolist() + df_model_copy['AWAY'].unique().tolist()

# Crear un diccionario de mapeo de equipos a valores numéricos únicos
team_mapping = {team: i for i, team in enumerate(all_teams)}

# Aplicar la codificación al DataFrame original
df_model_end = df_model_copy.replace({'HOME': team_mapping, 'AWAY': team_mapping})
df_model_end

# Features
X = df_model_end.drop(columns=['JOR', 'HOME', 'AWAY', '0_H', '1_H', '2_H', '3_H', '4_H', '5_H', '6_H', '7_H', '8_H', '9_H', '10_H', '11_H',
                               'G_H', 'G_A', '4_A', '5_A', '6_A', '7_A', '8_A', '9_A', '10_A', '11_A', '12_A', '13_A', '14_A', '15_A',
                               'DIS_REC_H', 'DIS_REC_A', 'TAR_AMA_H',
                               'TAR_AMA_A', 'TAR_ROJ_H', 'TAR_ROJ_A', 'FAL_REC_H', 'FAL_REC_A',
                               'FAL_COM_H', 'FAL_COM_A', 'PER_D_POS_H', 'PER_D_POS_A', 'REC_D_POS_H',
                               'REC_D_POS_A', 'FUE_DE_JUE_H', 'FUE_DE_JUE_A', 'DIS_REC_BLO_H',
                               'DIS_REC_BLO_A', 'SHO_H', 'SHO_A',
                               'H_WIN', 'A_WIN', 'DRAW', 'RESULT', 'AVG_%_POS_BAL_H', 'AVG_%_POS_BAL_A', 'AVG_SHO_TAR_H', 'AVG_SHO_TAR_A'])
                               # '12_H', '13_H', '14_H', '15_H',
                               # '0_A', '1_A', '2_A', '3_A',
                               # '%_POS_BAL_H', '%_POS_BAL_A', 'SHO_TAR_H', 'SHO_TAR_A',
                               # 'AVG_GOAL_H', 'AVG_GOAL_A',

# Etiquetas
y = df_model_end['RESULT']

# Número de tamaños diferentes para el tamaño del conjunto de prueba "test_size"
repeticiones = 10
for i_test_size in [ round(np.random.uniform(0.2, 0.3), 3) for i in range(repeticiones) ]:

  # Obtener resultado al escalar y normalizar las características
  for i_scaler in [True]: #, False]:

    '''
    ESTANDARIZACIÓN O NORMALIZACIÓN DE LAS CARACTERÍSTICAS
    '''
    # Conjutnos de datos de entrenamiento y priueba
    data_train_and_scaler = stand_norm_data(X=X, y=y, norm=i_scaler, test_size=i_test_size)
    X_train, X_test, y_train, y_test = data_train_and_scaler[0]
    scaler = data_train_and_scaler[1]

    '''
    MODELO FNN (FEEDFOWARD NEUROL NETWORK)
    '''
    # Crear un modelo secuencial
    model = Sequential()

    # Agregar capas al modelo
    # Capa oculta con 64 neuronas y activación ReLU
    model.add(Dense(64, input_dim=X.shape[1], activation="relu"))
    # Otra capa oculta con 32 neuronas y activación ReLU
    model.add(Dense(32, activation="relu"))
    # Capa de salida con 1 neurona y activación Sigmoide
    model.add(Dense(1, activation="sigmoid"))

    # Compilar el modelo
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])

    # Entrenar el modelo
    model.fit(X_train, y_train, epochs=80, batch_size=32, validation_data=(X_test, y_test), verbose=0)

    print('\nAcurracy TEST y TRAIN:')

    # Evaluación del modelo FNN con los datos test
    loss_test, accuracy_test = model.evaluate(X_test, y_test)
    # Evaluación del modelo FNN con los datos train
    loss_train, accuracy_train = model.evaluate(X_train, y_train)

    '''
    GUARDAR MODELO Y SCALER, usando TensorFlow, CON UN "acc" DE TEST MAYOR A 0.75
    '''
    # if accuracy_test >= 0.75:
    #   name_model = 'fnn_liga_col_ACCts_{:.3f}_ACCtr_{:.3f}_{}'.format(accuracy_test, accuracy_train,
    #                                                                   np.where(i_scaler==True, 'NORM', 'STD'))
    #   path_model = '{}/Modelos/Models H5/{}.h5'.format(path, name_model)

    #   # Guardar el modelo utilizando las funciones integradas de TensorFlow/Keras
    #   model.save(path_model)
    #   print("Model saved!")

    #   # Guardar el objeto scaler en un archivo en My Driver
    #   joblib.dump(scaler, '{}/Modelos/Scalers/scaler_ACCts_{:.3f}_ACCtr_{:.3f}_{}.pkl'.format(path, accuracy_test, accuracy_train,
    #                                                                                 np.where(i_scaler==True, 'NORM', 'STD')))
    #   print("Scaler saved!")



    # # print("Loss:", loss)
    # print('Scaler: {}. Test Size: {}'.format(np.where(i_scaler==True, 'NORM', 'STANDARD'), i_test_size))
    # # print("Accuracy:", accuracy)

    # # Predicción
    # predictions = model.predict(X_test)
    # print('predictions {}'.format(predictions))
    # print('\n')
    # # print(predictions)

"""## 4.3 **Cargar modelo guardado y predecir nueva data**"""

df_model['HOME'].sort_values().unique()

# Ruta del archivo del modelo guardado
path_model_load = '{}/Modelos/Models H5/'.format(path)
path_scaler_load = '{}/Modelos/Scalers/'.format(path)
name_model = '{}fnn_liga_col_ACCts_0.778_ACCtr_0.876_NORM.h5'.format(path_model_load)
# name_scaler = '{}fnn_liga_col_ACCts_0.778_ACCtr_0.876_NORM.pkl'.format(path_scaler_load)

def make_pred(index_away):
  # Nombre de los euqipos de la nueva data
  new_home = i_match
  new_away = teams_away[index_away]

  # New Features
  new_data = {'12_H': [], '13_H': [], '14_H': [], '15_H': [], '0_A': [], '1_A': [], '2_A': [], '3_A': [],
              '%_POS_BAL_H': [], '%_POS_BAL_A': [], 'SHO_TAR_H': [], 'SHO_TAR_A': [],
              'AVG_GOAL_H': [], 'AVG_GOAL_A': []}

  '''
  CARGAR NUEVA DATA
  '''
  for i_region in new_data.keys():
    if i_region[-1] == 'H':
      if len(i_region) < 5:
        new_data[i_region].append(ceil(df_model_copy[df_model_copy['HOME'] == new_home].groupby(['JOR', 'HOME'])[i_region].sum().mean()))
      elif i_region in ['%_POS_BAL_H', 'AVG_GOAL_H', 'SHO_TAR_H']:
        new_data[i_region].append(df_model_copy[ df_model_copy['HOME'] == new_home ].groupby(['JOR', 'HOME'])[i_region].sum().values[-1])

    else:
      if len(i_region) < 5:
        new_data[i_region].append(ceil(df_model_copy[df_model_copy['AWAY'] == new_away].groupby(['JOR', 'AWAY'])[i_region].sum().mean()))
      elif i_region in ['%_POS_BAL_A', 'AVG_GOAL_A','SHO_TAR_A']:
        new_data[i_region].append(df_model_copy[ df_model_copy['AWAY'] == new_away ].groupby(['JOR', 'AWAY'])[i_region].sum().values[-1])

  # Dataframe de la nueva data
  df_new_data = pd.DataFrame(new_data)

  '''
  ESCALAR LA NUEVA DATA DENTRO DEL MISMO RANGO DE ESCALAMENTO AL MODELO
  '''
  # Cargar el objeto scaler
  scaler = joblib.load('{}/scaler_{}.pkl'.format(path_scaler_load, name_model[-31:-3]))

  # Normalizar/Escalar los nuevos datos usando el scaler cargado
  X_new_norm = scaler.transform(df_new_data)

  '''
  CARGAR MODELO Y REALIZAR PREDICCIONES
  '''
  # Cargar el modelo
  loaded_model = load_model(name_model)

  # Realizar predicciones con el modelo cargado
  predictions = loaded_model.predict(X_new_norm)

  print('\nPredicción: {} Vs {}'.format(new_home.upper(), new_away.upper()))
  print('Con el modelo {}'.format(name_model[-44:]))
  print('Es de {}'.format(predictions))



grupos = {'GURPO_A': ['Atlético Bucaramanga', 'Deportivo Pereira', 'Junior', 'Millonarios'],
          'GURPO_B': ['La Equidad', 'Once Caldas', 'Santa Fe', 'Tolima']}

for i_key, i_val in grupos.items():
  print(i_key, i_val)
  for enu, i_match in enumerate(i_val):
    teams_away = i_val.copy()
    teams_away.pop(enu)
    make_pred(index_away=0)
    make_pred(index_away=1)
    make_pred(index_away=2)
    print('\n{}'.format('═'*90))

# # Ruta del archivo del modelo guardado
# path_model_load = '{}/Modelos/Models H5/'.format(path)
# path_scaler_load = '{}/Modelos/Scalers/'.format(path)

# # Nombre de los euqipos de la nueva data
# new_home = 'Atlético Bucaramanga'
# new_away = 'Junior'

# new_data = {'12_H': [], '13_H': [], '14_H': [], '15_H': [], '0_A': [], '1_A': [], '2_A': [], '3_A': [],
#             '%_POS_BAL_H': [], '%_POS_BAL_A': [], 'SHO_TAR_H': [], 'SHO_TAR_A': [],
#             'AVG_GOAL_H': [], 'AVG_GOAL_A': []}

# # Iterar sobre los archivos en el directorio
# for i_models in os.listdir(path_model_load):

#   # Ruta de un archivo ".h5" en particular
#   ruta_archivo = os.path.join(path_model_load, i_models)

#   # Verificar si "ruta_archivo" es un archivo
#   if os.path.isfile(ruta_archivo):
#     '''
#     CARGAR NUEVA DATA
#     '''
#     for i_region in new_data.keys():
#       if i_region[-1] == 'H':
#         if len(i_region) < 5:
#           new_data[i_region].append(ceil(df_model_copy[df_model_copy['HOME'] == new_home].groupby(['JOR', 'HOME'])[i_region].sum().mean()))
#         elif i_region in ['%_POS_BAL_H', 'AVG_GOAL_H', 'SHO_TAR_H']:
#           new_data[i_region].append(df_model_copy[ df_model_copy['HOME'] == new_home ].groupby(['JOR', 'HOME'])[i_region].sum().values[-1])

#       else:
#         if len(i_region) < 5:
#           new_data[i_region].append(ceil(df_model_copy[df_model_copy['AWAY'] == new_away].groupby(['JOR', 'AWAY'])[i_region].sum().mean()))
#         elif i_region in ['%_POS_BAL_A', 'AVG_GOAL_A','SHO_TAR_A']:
#           new_data[i_region].append(df_model_copy[ df_model_copy['AWAY'] == new_away ].groupby(['JOR', 'AWAY'])[i_region].sum().values[-1])

#     # Dataframe de la nueva data
#     df_new_data = pd.DataFrame(new_data)

#     '''
#     ESCALAR LA NUEVA DATA DENTRO DEL MISMO RANGO DE ESCALAMENTO AL MODELO
#     '''
#     # Cargar el objeto scaler
#     # print(i_models[-31:])
#     scaler = joblib.load('{}/scaler_{}.pkl'.format(path_scaler_load, i_models[-31:-3]))

#     # Normalizar/Escalar los nuevos datos usando el scaler cargado
#     X_new_norm = scaler.transform(df_new_data)

#     '''
#     CARGAR MODELO Y REALIZAR PREDICCIONES
#     '''
#     # Cargar el modelo
#     loaded_model = load_model(ruta_archivo)

#     # Realizar predicciones con el modelo cargado
#     predictions = loaded_model.predict(X_new_norm)

#     print('\nPredicción: {} Vs {}'.format(new_home, new_away))
#     print('Con el modelo {}'.format(i_models))
#     print('Es de {}'.format(predictions))


# # # Imprimir las predicciones
# # print(predictions)

# # # Acceder a la configuración del modelo
# # model_config = loaded_model.get_config()

# # # Imprimir la configuración del modelo
# # print(model_config)

# # # Acceder a la lista de métricas compiladas en el modelo
# # compiled_metrics = loaded_model.metrics_names

# # # Imprimir las métricas compiladas
# # print(compiled_metrics)